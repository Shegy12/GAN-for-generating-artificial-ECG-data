{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_research.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O85C7i3tEXGh",
        "VpQ-gECPoyol",
        "53fK8zX1IRiC",
        "gtuthROGTbcP"
      ],
      "authorship_tag": "ABX9TyO1qooGxIoZowXLjBVrI2ST",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shegy12/GAN-for-generating-artificial-ECG-data/blob/master/GAN_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2JiU3NDf31J",
        "colab_type": "text"
      },
      "source": [
        "# **RESEARCH OF METHODS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6nevz-BUt2A",
        "colab_type": "text"
      },
      "source": [
        "## **RGAN** (hard)\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/ratschlab/RGAN\n",
        "\n",
        "**Publication :** \n",
        "\n",
        "Recurrent (conditional) generative adversarial networks for generating real-valued time series data.\n",
        "\n",
        "https://arxiv.org/abs/1706.02633"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAw0S0XUf-Q7",
        "colab_type": "text"
      },
      "source": [
        "## **SR_CNN_VAE_GAN** (hard)\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/makbari7/SR-CNN-VAE-GAN\n",
        "\n",
        "**Publication :**\n",
        "\n",
        "Tensorflow implementation of Semi-recurrent CNN-based VAE-GAN for Sequential Data Generation\n",
        "\n",
        "https://arxiv.org/abs/1806.00509"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3L41G7LlNSK",
        "colab_type": "text"
      },
      "source": [
        "## **C_RNN_GAN** (hard)\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/olofmogren/c-rnn-gan\n",
        "\n",
        "**Publication :**\n",
        "\n",
        "C-RNN-GAN: Continuous recurrent neural networks with adversarial training \n",
        "\n",
        "http://mogren.one/publications/2016/c-rnn-gan/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2g7TJgdMN1J",
        "colab_type": "text"
      },
      "source": [
        "## **GAN_Time_Series** (tensorflow, hard)\n",
        "\n",
        "A model to generate time series data with the purpose of augmenting a dataset of various time series.\n",
        "\n",
        "The model is a Conditional Generative Adversarial Network for time series with not regular time intervals.\n",
        "\n",
        "The model is created to generate a new time series given a training set of them.\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/gioramponi/GAN_Time_Series\n",
        "\n",
        "**Publication :**\n",
        "\n",
        "https://arxiv.org/abs/1811.08295"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O85C7i3tEXGh",
        "colab_type": "text"
      },
      "source": [
        "##**InfoGAN** (tensorflow)\n",
        "\n",
        "A tensorflow implementation of GAN ( exactly InfoGAN or Info GAN ) to one dimensional ( 1D ) time series data. We've applied InfoGAN model to one dimensional time series data for classifying time series data through unsupervised way.\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/buriburisuri/timeseries_gan\n",
        "\n",
        "**Publication :**\n",
        "\n",
        "https://arxiv.org/abs/1606.03657\n",
        "\n",
        "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpQ-gECPoyol",
        "colab_type": "text"
      },
      "source": [
        "## **TSGAN** (confusing)\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/numancelik34/TimeSeries-GAN\n",
        "\n",
        "**Publication :** None\n",
        "\n",
        "Generation of Time Series data using generative adversarial networks (GANs) for biological purposes.\n",
        "\n",
        "1-D convolutional neural networks (CNNs) were used in the generator and discriminator of the GAN components to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53fK8zX1IRiC",
        "colab_type": "text"
      },
      "source": [
        "##**GAN that creates short audio clips**\n",
        "\n",
        "**GitHub :**\n",
        "\n",
        "https://github.com/djl12328/SignalGeneration\n",
        "\n",
        "**Publication :** None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGx459pIJNzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling1D, Conv1D, UpSampling2D, MaxPooling1D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import wave\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BatchGenerator():\n",
        "    def __init__(self, batch_size, directory):\n",
        "        self.MAX_SIZE = int.from_bytes(b\"\\xFF\\xFF\", \"little\")\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.directory = directory\n",
        "\n",
        "        self.data = self._load_data()\n",
        "\n",
        "\n",
        "    def fetch_random_batch(self):\n",
        "        np.random.shuffle(self.data)\n",
        "\n",
        "        return self.data[0:self.batch_size]\n",
        "    \n",
        "    def _convert_bytes_to_tensor(self, byte_list):\n",
        "        four_wide = [(byte_list[i*4:i*4+2], byte_list[i*4+2:i*4+4]) for i in range(len(byte_list) // 4)]\n",
        "\n",
        "        channel1_list = [int.from_bytes(four_wide[i][0], \"little\") / self.MAX_SIZE for i in range(len(four_wide))]\n",
        "        channel2_list = [int.from_bytes(four_wide[i][1], \"little\") / self.MAX_SIZE for i in range(len(four_wide))]\n",
        "        \n",
        "        signal_tensor = np.column_stack([channel1_list, channel2_list])\n",
        "\n",
        "        return signal_tensor\n",
        "    \n",
        "    def convert_tensor_to_bytes(self, tensor):\n",
        "        tensor *= self.MAX_SIZE\n",
        "        tensor = tensor.astype(np.int16)\n",
        "\n",
        "        byte_list = tensor.tobytes()\n",
        "\n",
        "        return byte_list\n",
        "\n",
        "    def _load_data(self):\n",
        "        data = []\n",
        "\n",
        "        first = True\n",
        "        for file in tqdm(os.listdir(self.directory)):\n",
        "            if file.endswith(\".wav\"):\n",
        "                wave_reader = wave.open(self.directory + file, \"rb\")\n",
        "\n",
        "                if first:\n",
        "                    self.channels = wave_reader.getnchannels()\n",
        "                    self.sampwidth = wave_reader.getsampwidth()\n",
        "                    self.framerate = wave_reader.getframerate()\n",
        "                    self.frames_per_sample = wave_reader.getnframes()\n",
        "                    first = False\n",
        "\n",
        "                byte_list = wave_reader.readframes(wave_reader.getnframes())\n",
        "                sample_tensor = self._convert_bytes_to_tensor(byte_list)\n",
        "\n",
        "                data.append(sample_tensor)\n",
        "        \n",
        "        return np.asarray(data)\n",
        "    \n",
        "    def save_bytes_to_wav(self, byte_list, filename):\n",
        "        wave_writer = wave.open(filename, \"wb\")\n",
        "\n",
        "        wave_writer.setnchannels(self.channels)\n",
        "        wave_writer.setsampwidth(self.sampwidth)\n",
        "        wave_writer.setframerate(self.framerate)\n",
        "        wave_writer.writeframes(byte_list)\n",
        "\n",
        "        wave_writer.close()\n",
        "\n",
        "class DCGAN():\n",
        "    def __init__(self, inputdir, outputdir, batch_size, interval, modelfile=None):\n",
        "        # Set up batch generator\n",
        "        self.bg = BatchGenerator(batch_size, inputdir)\n",
        "        self.batch_size = batch_size\n",
        "        self.inputdir = inputdir\n",
        "        self.outputdir = outputdir\n",
        "        self.interval = interval\n",
        "        self.modelfile = modelfile\n",
        "\n",
        "        # Input shape\n",
        "        self.sample_frames = 441000\n",
        "        self.channels = 2\n",
        "        self.sample_shape = (self.sample_frames, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        print(\"Discriminator built\")\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "        print(\"Generator built\")\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        \n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(1 * 11025, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((11025, 1)))\n",
        "        model.add(Conv1D(256, kernel_size=7, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Reshape((11025, 1, 256)))\n",
        "        model.add(UpSampling2D(size=(5, 1)))\n",
        "        model.add(Reshape((55125, 256)))\n",
        "        model.add(Conv1D(128, kernel_size=7, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Reshape((55125, 1, 128)))\n",
        "        model.add(UpSampling2D(size=(2, 1)))\n",
        "        model.add(Reshape((110250, 128)))\n",
        "        model.add(Conv1D(128, kernel_size=7, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Reshape((110250, 1, 128)))\n",
        "        model.add(UpSampling2D(size=(2, 1)))\n",
        "        model.add(Reshape((220500, 128)))\n",
        "        model.add(Conv1D(64, kernel_size=7, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Reshape((220500, 1, 64)))\n",
        "        model.add(UpSampling2D(size=(2, 1)))\n",
        "        model.add(Reshape((441000, 64)))\n",
        "        model.add(Conv1D(32, kernel_size=7, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv1D(self.channels, kernel_size=7, padding=\"same\"))\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv1D(32, kernel_size=3, strides=2, input_shape=self.sample_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv1D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv1D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv1D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(MaxPooling1D(pool_size=2, padding=\"valid\"))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.sample_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs):\n",
        "\n",
        "        #self.bg.data = np.expand_dims(self.bg.data, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((self.batch_size, 1))\n",
        "        fake = np.zeros((self.batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "\n",
        "            batch = self.bg.fetch_random_batch()\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
        "            gen_samples = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(batch, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_samples, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % self.interval == 0:\n",
        "                self.save_samples(epoch)\n",
        "\n",
        "    def save_samples(self, epoch):\n",
        "        n = 5\n",
        "        noise = np.random.normal(0, 1, (n, self.latent_dim))\n",
        "        gen_samples = self.generator.predict(noise)\n",
        "\n",
        "        for i in range(n):\n",
        "            tensor = gen_samples[i]\n",
        "            byte_list = self.bg.convert_tensor_to_bytes(tensor)\n",
        "            path = self.outputdir + \"sample{}\".format(epoch) + \"_{}.wav\".format(i)\n",
        "            self.bg.save_bytes_to_wav(byte_list, path)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"inputdir\", help=\"Directory containing audio samples\")\n",
        "    parser.add_argument(\"outputdir\", help=\"Directory to output samples during training\")\n",
        "    parser.add_argument(\"--batchsize\", type=int, default=8, help=\"Batch size used during training\")\n",
        "    parser.add_argument(\"--interval\", type=int, default=10, help=\"Interval between saving samples during training\")\n",
        "    parser.add_argument(\"--modelfile\", help=\"File to save model at after training\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1000, help=\"Number of training epochs\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    gan = DCGAN(args.inputdir, args.outputdir, args.batchsize, args.interval, args.modelfile)\n",
        "    gan.train(args.epochs)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtuthROGTbcP",
        "colab_type": "text"
      },
      "source": [
        "##**Simple GAN in Keras** (image not signal)\n",
        "\n",
        "https://pathmind.com/wiki/generative-adversarial-network-gan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3noo6RzrTh6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the generator\n",
        "        self.generator = self.build_generator()\n",
        "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(100,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The valid takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator) takes\n",
        "        # noise as input => generates images => determines validity\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        noise_shape = (100,)\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_shape=noise_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=noise_shape)\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=img_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        half_batch = int(batch_size / 2)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "            # Generate a half batch of new images\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "            # The generator wants the discriminator to label the generated samples\n",
        "            # as valid (ones)\n",
        "            valid_y = np.array([1] * batch_size)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, 100))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"gan/images/mnist_%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gan = GAN()\n",
        "    gan.train(epochs=30000, batch_size=32, save_interval=200)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}